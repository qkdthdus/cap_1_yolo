from ultralytics import YOLO
import cv2
import numpy as np
import time
import datetime
# ëª¨ë‹ˆí„° í•´ìƒë„ ì •ë³´ë¥¼ ê°€ì ¸ì˜¤ê¸° ìœ„í•´ tkinter ëª¨ë“ˆ ì‚¬ìš©
import tkinter as tk

# --------------------------
# ì „ì—­ ë§ˆìš°ìŠ¤ ìƒíƒœ ë³€ìˆ˜
# --------------------------
mouse_r_click_triggered = False

# --------------------------
# ë§ˆìš°ìŠ¤ ì½œë°± í•¨ìˆ˜
# --------------------------
def handle_mouse_event(event, x, y, flags, param):
    """ ë§ˆìš°ìŠ¤ ì´ë²¤íŠ¸ë¥¼ ì²˜ë¦¬í•˜ê³  ìš°í´ë¦­ ì‹œ í”Œë˜ê·¸ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤. """
    global mouse_r_click_triggered
    # ğŸ“¢ ë§ˆìš°ìŠ¤ ì˜¤ë¥¸ìª½ ë²„íŠ¼ì„ ëˆŒë €ì„ ë•Œ (cv2.EVENT_RBUTTONDOWN)
    if event == cv2.EVENT_RBUTTONDOWN:
        mouse_r_click_triggered = True
        print("=== ğŸ–±ï¸ ë§ˆìš°ìŠ¤ ìš°í´ë¦­ ìˆ˜ë™ íŠ¸ë¦¬ê±° í™œì„±í™”! ===")


# --------------------------
# ëª¨ë‹ˆí„° í•´ìƒë„ ê°€ì ¸ì˜¤ê¸° (Tkinter ì‚¬ìš©)
# --------------------------
try:
    # Tkinterì˜ Toplevel ìœˆë„ìš°ë¥¼ ì‚¬ìš©í•˜ì—¬ í™”ë©´ í•´ìƒë„ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
    root = tk.Tk()
    screen_width = root.winfo_screenwidth()
    screen_height = root.winfo_screenheight()
    root.destroy()
    print(f"ì‹œìŠ¤í…œ í•´ìƒë„ ê°ì§€: {screen_width}x{screen_height}")
except tk.TclError:
    print("ê²½ê³ : Tkinter ì´ˆê¸°í™” ì‹¤íŒ¨. ê¸°ë³¸ í•´ìƒë„ 1920x1080 ì‚¬ìš©.")
    screen_width = 1920
    screen_height = 1080

# --------------------------
# ì„¤ì • ê°’ (ëª¨ë‹ˆí„° í•´ìƒë„ ê¸°ë°˜ìœ¼ë¡œ ê³„ì‚°)
# --------------------------
VIDEO_FILES = [
    "./brand_pic/video1.mp4", 
    "./brand_pic/video2.mp4", 
    "./brand_pic/video3.mp4", 
    "./brand_pic/video4.mp4" 
]
TRIGGER_BOX_SIZE = 300
DEBUG_WINDOW_NAME = "Webcam Debug View (ESC to Quit)"

# ğŸ“¢ ì¶”ê°€: ì¿¨ë‹¤ìš´ ì„¤ì •
COOLDOWN_DURATION = 3.0 # 3ì´ˆ ì¿¨ë‹¤ìš´

# ì´ˆê¸° ëª¨ë‹ˆí„° ì°½ í¬ê¸°ë¥¼ í™”ë©´ í•´ìƒë„ ê¸°ë°˜ìœ¼ë¡œ ê³„ì‚°
MAX_ROW_SIZE = 2 # í•œ ì¤„ì— 2ê°œ ì°½ ë°°ì¹˜

# ë°°ì¹˜ ê°„ê²© ë° ì‹œì‘ ìœ„ì¹˜
MARGIN_X, MARGIN_Y = 20, 40 
START_X, START_Y = 50, 50

# 4ê°œ ì°½ì´ 2x2ë¡œ ë°°ì¹˜ë  ìˆ˜ ìˆëŠ” ìµœëŒ€ í¬ê¸° ê³„ì‚°
INITIAL_WINDOW_W = (screen_width - START_X - (MAX_ROW_SIZE + 1) * MARGIN_X) // MAX_ROW_SIZE
INITIAL_WINDOW_H = (screen_height - START_Y - (MAX_ROW_SIZE + 1) * MARGIN_Y) // MAX_ROW_SIZE

# ìµœì†Œ í¬ê¸° ì œí•œ (ë„ˆë¬´ ì‘ì•„ì§€ëŠ” ê²ƒì„ ë°©ì§€)
INITIAL_WINDOW_W = max(320, INITIAL_WINDOW_W)
INITIAL_WINDOW_H = max(180, INITIAL_WINDOW_H)

print(f"ê³„ì‚°ëœ ì´ˆê¸° ì°½ í¬ê¸°: {INITIAL_WINDOW_W}x{INITIAL_WINDOW_H}")

# --------------------------
# YOLO ëª¨ë¸ ë¡œë“œ (ìƒëµ)
# --------------------------
model = YOLO("yolov8n.pt") 

try:
    pose_model = YOLO("yolov8n-pose.pt")
except Exception as e:
    print("--- âš ï¸ ê²½ê³ : yolov8n-pose.pt ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨ âš ï¸ ---")
    print(f"ì˜¤ë¥˜: {e}")
    print("YOLO Pose ëª¨ë¸ì„ ë‹¤ìš´ë¡œë“œí•˜ì—¬ ìŠ¤í¬ë¦½íŠ¸ì™€ ê°™ì€ ê²½ë¡œì— ë‘ì‹­ì‹œì˜¤.")
    pose_model = None 

# --------------------------
# í—¬í¼ í•¨ìˆ˜ (ì´ì „ê³¼ ë™ì¼)
# --------------------------

def detect_person(frame):
    close = False
    boxes = []
    results = model(frame, classes=0, verbose=False) 
    for r in results:
        for det in r.boxes:
            x1, y1, x2, y2 = map(int, det.xyxy[0].tolist())
            boxes.append((x1, y1, x2, y2))
            w = x2 - x1
            h = y2 - y1
            if w >= TRIGGER_BOX_SIZE or h >= TRIGGER_BOX_SIZE: 
                close = True
    return close, boxes

# ğŸ“¢ ì£¼ì˜: ì´ í•¨ìˆ˜ëŠ” í”„ë ˆì„ì— ì§ì ‘ ì‹œê°í™”ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.
def get_hand_status_pose(frame):
    if pose_model is None:
        return False, False

    # Pose ê°ì§€ ì‹œ, í”„ë ˆì„ì´ ì´ë¯¸ ì¢Œìš° ë°˜ì „ëœ ìƒíƒœì¸ì§€ ê³ ë ¤í•´ì•¼ í•©ë‹ˆë‹¤.
    # YOLO ì¶”ë¡ ì€ ì›ë³¸ í”„ë ˆì„(ë°˜ì „ë˜ì§€ ì•Šì€)ìœ¼ë¡œ ì‹¤í–‰í•˜ëŠ” ê²ƒì´ ì •í™•ë„ë¥¼ ë†’ì¼ ìˆ˜ ìˆì§€ë§Œ,
    # í˜„ì¬ êµ¬ì¡°ìƒ ë°˜ì „ëœ í”„ë ˆì„ì´ ì „ë‹¬ë  ê²ƒì´ë¯€ë¡œ, PoseëŠ” ë°˜ì „ëœ ì´ë¯¸ì§€ ì¢Œí‘œì— ë§ì¶° ì‹œê°í™”í•©ë‹ˆë‹¤.
    pose_results = pose_model(frame, verbose=False)
    
    WRIST_KPTS = [9, 10]
    ELBOW_KPTS = [7, 8]
    CONF_THRESHOLD = 0.5 
    MIN_DISTANCE = 50 

    hand_is_open = False
    hand_is_closed = False

    for r in pose_results:
        if r.keypoints is None or r.keypoints.data.numel() == 0:
            continue
            
        kpts = r.keypoints.data[0].cpu().numpy() 
        if kpts.shape[0] < 17: continue
        
        h, w = frame.shape[:2]
        if r.boxes and r.boxes.xyxy.numel() > 0:
             x1, y1, x2, y2 = map(int, r.boxes.xyxy[0].tolist())
             person_center_x = (x1 + x2) // 2
             cam_center_x = w // 2
             if abs(person_center_x - cam_center_x) > w * 0.4: continue


        for wrist_idx, elbow_idx in zip(WRIST_KPTS, ELBOW_KPTS):
            wrist_kpt = kpts[wrist_idx]
            elbow_kpt = kpts[elbow_idx]
            
            if wrist_kpt[2] > CONF_THRESHOLD and elbow_kpt[2] > CONF_THRESHOLD:
                
                wrist_pos = np.array([wrist_kpt[0], wrist_kpt[1]])
                elbow_pos = np.array([elbow_kpt[0], elbow_kpt[1]])
                distance = np.linalg.norm(wrist_pos - elbow_pos)
                
                if distance > MIN_DISTANCE:
                    hand_is_open = True
                    cv2.circle(frame, (int(wrist_kpt[0]), int(wrist_kpt[1])), 8, (0, 255, 0), -1) 
                else:
                    hand_is_closed = True
                    cv2.circle(frame, (int(wrist_kpt[0]), int(wrist_kpt[1])), 8, (0, 0, 255), -1)

    return hand_is_closed, hand_is_open


# --------------------------
# 1ï¸âƒ£ OpenCV ì°½ ìƒì„± ë° ì´ˆê¸°í™” 
# --------------------------
screen_units = []

current_x = START_X 
current_y = START_Y
max_h_in_row = 0

for i in range(len(VIDEO_FILES)):
    window_name = f"Video Monitor {i+1}"
    
    cap_vid = cv2.VideoCapture(VIDEO_FILES[i])
    if not cap_vid.isOpened():
         raise Exception(f"ì˜ìƒ ë¶ˆëŸ¬ì˜¤ê¸° ì‹¤íŒ¨: {VIDEO_FILES[i]}")

    W_orig = int(cap_vid.get(cv2.CAP_PROP_FRAME_WIDTH))
    H_orig = int(cap_vid.get(cv2.CAP_PROP_FRAME_HEIGHT))
    
    ret, first_frame = cap_vid.read()
    cap_vid.release()

    if not ret:
        raise Exception(f"ì˜ìƒ ë¶ˆëŸ¬ì˜¤ê¸° ì‹¤íŒ¨: {VIDEO_FILES[i]}")

    # 1. ì°½ ë°°ì¹˜ ìœ„ì¹˜ ê³„ì‚° 
    if i % MAX_ROW_SIZE == 0 and i > 0: 
        current_y += INITIAL_WINDOW_H + MARGIN_Y
        current_x = START_X
        max_h_in_row = 0

    # 2. OpenCV ì°½ ìƒì„± ë° ìœ„ì¹˜/í¬ê¸° ì§€ì • 
    cv2.namedWindow(window_name, cv2.WINDOW_NORMAL) 
    
    # ê³„ì‚°ëœ INITIAL_WINDOW_W/Hë¡œ í¬ê¸° ì„¤ì •
    cv2.resizeWindow(window_name, INITIAL_WINDOW_W, INITIAL_WINDOW_H) 
    cv2.moveWindow(window_name, current_x, current_y) 
    
    # ğŸ“¢ Video Monitor 1ì— ë§ˆìš°ìŠ¤ ì½œë°± ì„¤ì •
    if i == 0:
        cv2.setMouseCallback(window_name, handle_mouse_event) 


    # ë‹¤ìŒ ì°½ ìœ„ì¹˜ ì—…ë°ì´íŠ¸
    current_x += INITIAL_WINDOW_W + MARGIN_X
    max_h_in_row = max(max_h_in_row, INITIAL_WINDOW_H) 


    screen_units.append({
        "win_name": window_name,
        "active": False,
        "first_frame": first_frame,
        "video_path": VIDEO_FILES[i],
        "video_cap": None,
        "fps": 30,
        "delay_ms": 1, 
        "index": i,
        "width_orig": W_orig,     
        "height_orig": H_orig,    
        "initial_w": INITIAL_WINDOW_W, 
        "initial_h": INITIAL_WINDOW_H,
    })

# ì´ˆê¸° í™”ë©´ ì •ì§€ ìƒíƒœ í‘œì‹œ
for unit in screen_units:
    # ì •ì§€ í™”ë©´ í”„ë ˆì„ì„ ê³„ì‚°ëœ ì´ˆê¸° ì„¤ì • í¬ê¸°ë¡œ ë¦¬ì‚¬ì´ì§•í•˜ì—¬ í‘œì‹œ
    resized_frame = cv2.resize(unit["first_frame"], (unit["initial_w"], unit["initial_h"]))
    cv2.imshow(unit["win_name"], resized_frame)

# ë””ë²„ê·¸ ì°½ ìƒì„± ë° ë°°ì¹˜ (NORMAL ìœ ì§€)
cv2.namedWindow(DEBUG_WINDOW_NAME, cv2.WINDOW_NORMAL)

dbg_w, dbg_h = 640, 360
dbg_pos_x = current_x 
dbg_pos_y = START_Y 

cv2.resizeWindow(DEBUG_WINDOW_NAME, dbg_w, dbg_h)
cv2.moveWindow(DEBUG_WINDOW_NAME, dbg_pos_x, dbg_pos_y)


# --------------------------
# 2ï¸âƒ£ ì›¹ìº  ë° ë©”ì¸ ë£¨í”„ 
# --------------------------
cap = cv2.VideoCapture(0)
running = True

cooldown_end_time = 0.0

frame_counter = 0
INFERENCE_FREQUENCY = 20 

person_close = False
boxes = []
hand_is_closed = False
hand_is_open = False
hand_was_closed = False 

while running:
    frame_counter += 1
    
    ret, frame_raw = cap.read() # ğŸ“¢ ì›ë³¸ í”„ë ˆì„ ì½ê¸°
    if not ret: break
    
    # ğŸ“¢ ì¢Œìš° ë°˜ì „ ì ìš© (1: ì¢Œìš° ë°˜ì „)
    frame = cv2.flip(frame_raw, 1) 
    
    
    # ğŸ“¢ ì¡°ê±´ë¶€ YOLO ì¶”ë¡ 
    if frame_counter % INFERENCE_FREQUENCY == 0:
        # ì°¸ê³ : detect_personê³¼ get_hand_status_poseëŠ” ë°˜ì „ëœ 'frame'ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
        person_close_current, boxes_current = detect_person(frame) 
        hand_is_closed_current, hand_is_open_current = get_hand_status_pose(frame) 
        
        person_close = person_close_current
        boxes = boxes_current
        hand_is_closed = hand_is_closed_current
        hand_is_open = hand_is_open_current
    
    
    # ì¿¨ë‹¤ìš´ ìƒíƒœ í™•ì¸
    can_trigger = time.time() > cooldown_end_time
    
    # ìµœì¢… íŠ¸ë¦¬ê±° ë¡œì§
    detection_trigger = person_close and hand_is_open and hand_was_closed and can_trigger
    mouse_trigger = mouse_r_click_triggered and can_trigger
    
    trigger = detection_trigger or mouse_trigger
    
    hand_was_closed = hand_is_closed
    
    
    # 5. ëª¨ë‹ˆí„° ê°œë³„ ì²˜ë¦¬ (íŠ¸ë¦¬ê±° ë¡œì§ ì ìš©)
    
    # ë§ˆìš°ìŠ¤ íŠ¸ë¦¬ê±° ì´ˆê¸°í™”
    if mouse_r_click_triggered:
        mouse_r_click_triggered = False
        
    for unit in screen_units:
        
        # A) íŠ¸ë¦¬ê±° ë°œìƒ â†’ ë¹„ë””ì˜¤ ì‹œì‘
        if trigger and not unit["active"]:
            unit["active"] = True
            unit["video_cap"] = cv2.VideoCapture(unit["video_path"])
            fps = unit["video_cap"].get(cv2.CAP_PROP_FPS)
            unit["fps"] = fps if fps > 0 else 30 
            
            unit["delay_ms"] = max(1, int(1000 / unit["fps"])) 


        # B) ì¬ìƒ ì¤‘ì´ë©´ í”„ë ˆì„ ì½ê¸°
        if unit["active"]:
            ret_vid, vid_frame = unit["video_cap"].read()

            if ret_vid:
                w_current = cv2.getWindowImageRect(unit["win_name"])[2]
                h_current = cv2.getWindowImageRect(unit["win_name"])[3]

                if w_current > 0 and h_current > 0:
                    vid_frame = cv2.resize(vid_frame, (w_current, h_current))
                    
                cv2.imshow(unit["win_name"], vid_frame) 
                
            else:
                # ì˜ìƒ ë â†’ ì •ì§€ í™”ë©´ ë³µê·€
                unit["active"] = False
                if unit["video_cap"]:
                    unit["video_cap"].release()
                
                # ğŸ“¢ ì¿¨ë‹¤ìš´ ì„¤ì •: ëª¨ë“  ë¹„ë””ì˜¤ê°€ ëë‚˜ì•¼ë§Œ ì¿¨ë‹¤ìš´ ì‹œì‘
                if not any(u["active"] for u in screen_units):
                     cooldown_end_time = time.time() + COOLDOWN_DURATION
                     print(f"=== ğŸ¥¶ ì¿¨ë‹¤ìš´ ì‹œì‘: {COOLDOWN_DURATION}ì´ˆ ë™ì•ˆ ì¸ì‹ ì¤‘ì§€ ===")
                
                w_current = cv2.getWindowImageRect(unit["win_name"])[2]
                h_current = cv2.getWindowImageRect(unit["win_name"])[3]

                if w_current > 0 and h_current > 0:
                     resized_frame = cv2.resize(unit["first_frame"], (w_current, h_current))
                else:
                     resized_frame = cv2.resize(unit["first_frame"], (unit["initial_w"], unit["initial_h"]))

                cv2.imshow(unit["win_name"], resized_frame)

        # C) ì¬ìƒ ì¤‘ ì•„ë‹ˆë©´ ì²« í™”ë©´ ìœ ì§€ 
        elif not unit["active"]:
             w_current = cv2.getWindowImageRect(unit["win_name"])[2]
             h_current = cv2.getWindowImageRect(unit["win_name"])[3]
             
             if w_current != unit["initial_w"] or h_current != unit["initial_h"]:
                 if w_current > 0 and h_current > 0:
                    resized_frame = cv2.resize(unit["first_frame"], (w_current, h_current))
                    cv2.imshow(unit["win_name"], resized_frame)
    
    # 6. ë””ë²„ê¹… í™”ë©´ ë° í‚¤ ì…ë ¥ ì²˜ë¦¬ 
    dbg = frame.copy() # ğŸ“¢ ë°˜ì „ëœ í”„ë ˆì„ì„ ë³µì‚¬í•˜ì—¬ ì‚¬ìš©
    h_cam, w_cam = dbg.shape[:2]
    text_y_start = h_cam - 130
    
    # ì¿¨ë‹¤ìš´ ë‚¨ì€ ì‹œê°„ ê³„ì‚°
    remaining_cooldown = max(0.0, cooldown_end_time - time.time())

    for x1, y1, x2, y2 in boxes: 
        box_color = (0, 255, 255) if (x2 - x1) >= TRIGGER_BOX_SIZE or (y2 - y1) >= TRIGGER_BOX_SIZE else (255, 0, 0)
        cv2.rectangle(dbg, (x1, y1), (x2, y2), box_color, 2)

    person_color = (0, 255, 255) if person_close else (100, 100, 100)
    open_color = (0, 255, 0) if hand_is_open and can_trigger else (0, 100, 100)
    was_closed_color = (0, 165, 255) if hand_was_closed else (100, 100, 100)

    cv2.putText(dbg, f"1. CLOSE: {person_close}", (10, text_y_start), cv2.FONT_HERSHEY_SIMPLEX, 0.6, person_color, 2)
    cv2.putText(dbg, f"2. (WAS) CLOSED: {hand_was_closed}", (10, text_y_start + 30), cv2.FONT_HERSHEY_SIMPLEX, 0.6, was_closed_color, 2)
    cv2.putText(dbg, f"3. (IS) OPEN: {hand_is_open}", (10, text_y_start + 60), cv2.FONT_HERSHEY_SIMPLEX, 0.6, open_color, 2)
    cv2.putText(dbg, f"INFERENCE: 1/{INFERENCE_FREQUENCY} Frames (MAX Speed)", (10, text_y_start + 90), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 0), 2)


    if trigger:
        trigger_color = (0, 0, 255)
        trigger_text = "ğŸŸ¢ TRIGGER ACTIVATED! (Dual Trigger)" 
    elif remaining_cooldown > 0:
        trigger_color = (128, 128, 128) # íšŒìƒ‰
        trigger_text = f"â³ COOLDOWN: {remaining_cooldown:.1f}s"
    elif mouse_r_click_triggered:
        trigger_color = (255, 165, 0) # ì£¼í™©ìƒ‰ (ë§ˆìš°ìŠ¤ ëŒ€ê¸°)
        trigger_text = "WAITING FOR MOUSE TRIGGER"
    else:
        trigger_color = (255, 255, 255)
        trigger_text = "ğŸ”´ TRIGGER STANDBY"

    cv2.putText(dbg, trigger_text, (10, h_cam - 20), cv2.FONT_HERSHEY_SIMPLEX, 0.7, trigger_color, 2)

    cv2.imshow(DEBUG_WINDOW_NAME, cv2.resize(dbg, (dbg_w, dbg_h)))
    
    
    key = cv2.waitKey(1) 
    
    if key & 0xFF == 27: # ESC ì¢…ë£Œ
        running = False
        
    if cv2.getWindowProperty(DEBUG_WINDOW_NAME, cv2.WND_PROP_VISIBLE) < 1:
        running = False


# --------------------------
# ì¢…ë£Œ
# --------------------------
cap.release()
cv2.destroyAllWindows()